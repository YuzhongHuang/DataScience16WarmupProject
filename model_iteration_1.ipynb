{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Kaggle (Model Iteration 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to learn the basics of kaggle, I start this first model iteration of the Titanic warmup project. Basically, I followed along the tutorial in https://www.dataquest.io/mission/74/getting-started-with-kaggle\n",
    "and created a simple linear regression and logistic regression to have a simple machine learning model for the survival data in Titanic event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 1\n",
    "A good first step is to think logically about the columns and what we're trying to predict. What variables might logically affect the outcome of survived? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "titanic = pd.read_csv(\"train.csv\")\n",
    "titanic_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we look at the data, and did some exploring of the relationships between variables. We know that women and children were more likely to survive. Thus, Age and Sex are probably good predictors. It's also logical to think that passenger class might affect the outcome, as first class cabins were closer to the deck of the ship. Fare is tied to passenger class, and will probably be highly correlated with it, but might add some additional information. Number of siblings and parents/children will probably be correlated with survival one way or the other, as either there are more people to help you, or more people to think about and try to save."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptor\n",
    "A good second step is to look at high level descriptors of the data. In this case, we can use the pandas .describe() method to look at different characteristics of each numeric column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex  Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male   22      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female   38      1   \n",
      "2                             Heikkinen, Miss. Laina  female   26      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female   35      1   \n",
      "4                           Allen, Mr. William Henry    male   35      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    }
   ],
   "source": [
    "def Describe(df):\n",
    "    \"\"\"\n",
    "    Prints out abstract description for all variables in a data frame\n",
    "    \"\"\"\n",
    "    # Print the first 5 rows of the dataframe.\n",
    "    print(df.head(5))\n",
    "    print(df.describe())\n",
    "\n",
    "Describe(titanic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value\n",
    "As we can see from the result of describe(), when you used .describe() on the titanic dataframe in the last screen, you might have noticed that the Age column has a count of 714 when all the other columns have a count of 891. This indicates that there are missing values in the Age column -- the count is of non-missing (null, NA, or not a number) values. The data isn't perfect, we need to clean the data ourselves. Here, we will place the Nan in age column with the median of age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The titanic variable is available here.\n",
    "titanic[\"Age\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "Several of our columns are non-numeric, which is a problem when it comes time to make predictions -- we can't feed non-numeric columns into a machine learning algorithm and expect it to make sense of them.\n",
    "\n",
    "We will transform sex to 0 for male and 1 for female; change Embarked column from \"S\", \"C\", \"Q\" to 0, 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'female']\n"
     ]
    }
   ],
   "source": [
    "# Find all the unique genders -- the column appears to contain only male and female.\n",
    "print(titanic[\"Sex\"].unique())\n",
    "\n",
    "# Replace all the occurences of male with the number 0.\n",
    "titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "titanic.loc[titanic[\"Sex\"] == \"female\", \"Sex\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S' 'C' 'Q' nan]\n"
     ]
    }
   ],
   "source": [
    "# Find all the unique values for \"Embarked\".\n",
    "print(titanic[\"Embarked\"].unique())\n",
    "\n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "titanic.loc[titanic[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic.loc[titanic[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic.loc[titanic[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "We can now use linear regression to make predictions on our training set. In order to avoid overfitting, I will use cross validation. To cross validate, you split your data into some number of parts. This way, we generate predictions for the whole dataset without ever evaluating accuracy on the same data we train our model using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "We can use the excellent scikit-learn library to make predictions. We'll use a helper from sklearn to split the data up into cross validation folds, and then train an algorithm for each fold, and make predictions. At the end, we'll have a list of predictions, with each list item containing predictions for the corresponding fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the linear regression class\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Sklearn also has a helper that makes it easy to do cross validation\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# The columns we'll use to predict the target\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm class\n",
    "alg = LinearRegression()\n",
    "# Generate cross validation folds for the titanic dataset.  It return the row indices corresponding to train and test.\n",
    "# We set random_state to ensure we get the same splits every time we run this.\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds.\n",
    "    train_predictors = (titanic[predictors].iloc[train,:])\n",
    "    # The target we're using to train the algorithm.\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    # Training the algorithm using the predictors and target.\n",
    "    alg.fit(train_predictors, train_target)\n",
    "    # We can now make predictions on the test fold\n",
    "    test_predictions = alg.predict(titanic[predictors].iloc[test,:])\n",
    "    predictions.append(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating error\n",
    "Now that we have predictions, we can evaluate our error.\n",
    "\n",
    "We'll first need to define an error metric, so we can figure out how accurate our model is. From the Kaggle competition description, the error metric is percentage of correct predictions. We'll use this same metric to evaluate our performance locally.\n",
    "\n",
    "Specifically, I will figure out what proportion of the values in predictions are the exact same as the values in titanic[\"Survived\"]. This calculation should be left as a float (decimal) and assigned to the variable accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.783389450056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuzhong/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:11: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The predictions are in three separate numpy arrays.  Concatenate them into one.  \n",
    "# We concatenate them on axis 0, as they only have one axis.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Map predictions to outcomes (only possible outcomes are 1 and 0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions <=.5] = 0\n",
    "\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print \"Accuracy: \", accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "In the next phase, we will try logistic regression to make a prediction. Sklearn has a class for logistic regression that we can use. We'll also make things easier by using an sklearn helper function to do all of our cross validation and evaluation for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787878787879\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize our algorithm\n",
    "alg = LogisticRegression(random_state=1)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the test data\n",
    "We'll also need to replace a missing value in the Fare column. Use .fillna with the median of the column in the test set to replace this. There are no missing values in the Fare column of the training set, but test sets can sometimes be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic_test[\"Age\"] = titanic_test[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "titanic_test[\"Fare\"] = titanic_test[\"Fare\"].fillna(titanic_test[\"Fare\"].median())\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"male\", \"Sex\"] = 0 \n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "titanic_test[\"Embarked\"] = titanic_test[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the first Submission\n",
    "Create a new dataframe with only the columns Kaggle wants from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize the algorithm class\n",
    "alg = LogisticRegression(random_state=1)\n",
    "\n",
    "# Train the algorithm using all the training data\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Make predictions using the test set.\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "# Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "\n",
    "# submission.to_csv(\"titanic_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After submitting the file, I get an accuracy of 0.75120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisions\n",
    "\n",
    "As we can see the score is not very high, which means the model still can be improved greatly. In the model iteration 1, I will add two revision to the model to make it more accruate in predicting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new feature of ThirdClassMale\n",
    "\n",
    "The rationale behind that revision is that third class males has a very low rate of survival (about 13%). Seperating this attribute may increase the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find all the unique values for \"Pclass\".\n",
    "titanic[\"ThirdClassMale\"] = 0\n",
    "\n",
    "conditions = (titanic[\"Pclass\"] == 3) & (titanic[\"Sex\"] == 0)\n",
    "titanic.loc[conditions, \"ThirdClassMale\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same change for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find all the unique values for \"Pclass\".\n",
    "titanic_test[\"ThirdClassMale\"] = 0\n",
    "\n",
    "conditions = (titanic_test[\"Pclass\"] == 3) & (titanic_test[\"Sex\"] == 0)\n",
    "titanic_test.loc[conditions, \"ThirdClassMale\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a new feature of UpperClassFemale\n",
    "From my data exploration, I figured out that women from pClass 1 and 2 has above 90% survival rate. By seperating that fact into a new feature, I hope to make my model more accurate when predicting upper class females."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find all the unique values for \"Pclass\".\n",
    "titanic[\"UpperClassFemale\"] = 0\n",
    "\n",
    "conditions = (titanic[\"Pclass\"] == 1) | (titanic[\"Pclass\"] == 2) & (titanic[\"Sex\"] == 1)\n",
    "titanic.loc[conditions, \"UpperClassFemale\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same change for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find all the unique values for \"Pclass\".\n",
    "titanic_test[\"UpperClassFemale\"] = 0\n",
    "\n",
    "conditions = (titanic_test[\"Pclass\"] == 1) | (titanic_test[\"Pclass\"] == 2) & (titanic_test[\"Sex\"] == 1)\n",
    "titanic_test.loc[conditions, \"UpperClassFemale\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'UpperClassFemale', 'ThirdClassMale']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a random forest\n",
    "\n",
    "Make cross validated predictions for the titanic dataframe (which has already been loaded in). Use 3 folds. Use the random forest algorithm stored in alg to do the cross validation. Use the predictors to predict the Survived column. Assign the result to scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.814814814815\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize our algorithm with the default paramters\n",
    "# n_estimators is the number of trees we want to make\n",
    "# min_samples_split is the minimum number of rows we need to make a split\n",
    "# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Tuning\n",
    "The score in train dataset is increased significantly. However, when I tried to submit that model to Kaggle, score drops by 2 to 3 percent. The reason is overfitting and indeed, overfitting is a major problem in random forest model. Thing we can do to improve the accuracy of the random forest is to increase the number of trees we're using. Training more trees will take more time, but because of the fact that we're averaging many predictions made on different subsets of the data, having more trees will increase accuracy greatly (up to a point).\n",
    "\n",
    "The first thing I will try is to tune the parameters \"min_samples_split\" and \"min_samples_leaf\". Increasing the value of these parameters will reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.821548821549\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=4, min_samples_leaf=2)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, reducing overfitting increase the score in cross section, but the score for test data is still relatively low for Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating new features\n",
    "Here I will combine SibSp and Parch to a new feature \"\"FamilySize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]\n",
    "titanic_test[\"FamilySize\"] = titanic_test[\"SibSp\"] + titanic_test[\"Parch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the predictors and do a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = ['Pclass', 'Sex', 'Age', 'FamilySize', 'Fare', 'Embarked', 'UpperClassFemale', 'ThirdClassMale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.821548821549\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=4, min_samples_leaf=2)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's very interesting, it seems that combining two variables won't affect the random forcast algorithm at all.\n",
    "\n",
    "Then, I extract the title of the passenger from their name. The titles take the form of Master., Mr., Mrs.. There are a few very commonly used titles, and a \"long tail\" of one-off titles that only one or two passengers have.\n",
    "\n",
    "We'll first extract the titles with a regular expression, and then map each unique title to an integer value.\n",
    "\n",
    "We'll then have a numeric column that corresponds to the appropriate Title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# A function to get the title from a name.\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Get all the titles and print how often each one occurs.\n",
    "titles = titanic[\"Name\"].apply(get_title)\n",
    "titles_test = titanic_test[\"Name\"].apply(get_title)\n",
    "\n",
    "# Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "    titles_test[titles_test == k] = v\n",
    "\n",
    "# Add in the title column.\n",
    "titanic[\"Title\"] = titles\n",
    "titanic_test[\"Title\"] = titles_test\n",
    "titanic_test.loc[titanic_test[\"Title\"] == 'Dona', \"Title\"] = 2\n",
    "\n",
    "# Update the predictors\n",
    "predictors = ['Pclass', 'Sex', 'Age', 'FamilySize', 'Fare', 'Embarked', 'UpperClassFemale', 'ThirdClassMale', 'Title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best features\n",
    "To evaluate features, I will use univariate feature selection. This essentially goes column by column, and figures out which columns correlate most closely with what we're trying to predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAFTCAYAAADycV3/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKFdGqYiKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe0bae859d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling\n",
    "\n",
    "In this case, we'll ensemble logistic regression trained on the most linear predictors (the ones that have a linear ordering, and some correlation to Survived), and a gradient boosted tree trained on all of the predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.806958473625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuzhong/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:39: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Update the predictors\n",
    "predictors = ['Pclass', 'Sex', 'Fare', 'UpperClassFemale', 'ThirdClassMale', 'Title']\n",
    "\n",
    "# The algorithms we want to ensemble.\n",
    "# We're using the more linear predictors for the logistic regression, and everything with the gradient boosting classifier.\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors],\n",
    "    [LogisticRegression(random_state=1), predictors]\n",
    "]\n",
    "\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import BernoulliRBM\n",
    "\n",
    "alg_test = BernoulliRBM(n_components=256, learning_rate=0.1, batch_size=10, n_iter=10, verbose=0, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 9 features per sample; expecting 6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-a7226ec83100>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# Make predictions using the test set.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitanic_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Create a new dataframe with only the columns Kaggle wants from the dataset.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/yuzhong/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/base.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \"\"\"\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/yuzhong/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/base.pyc\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 249\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 9 features per sample; expecting 6"
     ]
    }
   ],
   "source": [
    "# full_predictions = []\n",
    "# for alg, predictors in algorithms:\n",
    "#     # Fit the algorithm using the full training data.\n",
    "#     alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "#     # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "#     predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "#     full_predictions.append(predictions)\n",
    "\n",
    "# # The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "# predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "\n",
    "# # Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "# predictions[predictions <= .5] = 0\n",
    "# predictions[predictions > .5] = 1\n",
    "# predictions = predictions.astype(int)\n",
    "\n",
    "# Update the predictors\n",
    "predictors = ['Pclass', 'Sex', 'Age', 'FamilySize', 'Fare', 'Embarked', 'UpperClassFemale', 'ThirdClassMale', 'Title']\n",
    "\n",
    "# Train the algorithm using all the training data\n",
    "alg_test.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Make predictions using the test set.\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "# Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"titanic_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result and Next Step\n",
    "\n",
    "After I upload my revised submission. I get a score of 0.78033, which is improved by 2.5%.\n",
    "\n",
    "My model still need a lots of improvement, including changing the number in age, since obviously, age for survival is absolutely not linear. There is a interesting right-skewed distribution, I might want to apply this model to my data. Also, I will try seperating women and men group. Treat them as different set of datas can machine learning them separately"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
